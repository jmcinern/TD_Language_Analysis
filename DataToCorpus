import datetime
import os
import re
import spacy
import pandas as pd


class DataToCorpus:

    def __init__(self, data_location):
        #Get paper names based on file names
        paper_names = []
        # Repository where papers are stored.
        data_repo = "C:/Users/josep/PycharmProjects/TD_Language_Analysis/DATA"
        file_names = os.listdir(data_repo)
        for fn in file_names:
            paper = fn
            paper_names.append(paper)

        self.data_location = data_location
        # List of paper names
        self.paper_names = paper_names

    def _is_month(self, word):
        # Check if the word represents a month
        english_months = [
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december'
        ]

        return word.lower() in english_months

    def _is_year(self, word):
        # Check if the word represents a year
        return word.isdigit() and len(word) == 4

    def tokenise(self, text, nlp):
        doc = nlp(text)
        tokens = []

        for token in doc:
            if not token.is_space:
                #print(token.pos_)
                tokens.append(token.text.lower())
        return tokens

    def _files_to_corpus(self, file_name, nlp):
        # Use list of France related words to only add articles with these words in title.
        varadkar_related_words = ['leo', 'taoiseach', 'varadkar']
        title_list = []
        file_path = os.path.join(self.data_location, file_name)

        # Dictionary to map French month names to numeric values
        en_month_mapping = {
            'january': 1, 'february': 2, 'march': 3, 'april': 4,
            'may': 5, 'june': 6, 'july': 7, 'august': 8,
            'september': 9, 'october': 10, 'november': 11, 'december': 12
        }

        # Create a list to store articles
        articles = []

        # Open the TXT file for reading
        with open(file_path, 'r', encoding='utf-8') as file:
            varadkar_related_title = False
            duplicate_title = True
            lines = file.readlines()
            num_lines = len(lines)

            # Initialize variables for the current article
            text = []
            # Bool to signify that we are in the body of the text
            in_body = False
            newspaper = ""
            date = ""
            num_tokens = ""
            title = ""

            # Iterate through lines in the file
            for line_index, line in enumerate(lines):
                line = line.strip()

                # Get number of tokens of article, format: Length: 999 words
                pattern = r'Length: (\d+) words'
                match = re.match(pattern, line)
                if match:
                    num_tokens = int(match.group(1))

                # Check if the line contains the name of any paper
                if any(paper_name == line for paper_name in self.paper_names):
                    # Set the title as the line before the newspaper
                    title_index = max(0, line_index - 1)
                    title = lines[title_index].strip()
                    if title == "":
                        title_index = max(0, line_index - 2)
                        title = lines[title_index].strip()
                        title_list.append(title)
                    newspaper = line

                    # Move to the next line to get the date
                    next_index = min(line_index + 1, num_lines - 1)
                    line = lines[next_index].strip()

                    # Initialize month, day, and year
                    month, day_str, year_str = None, None, None

                    # Check each word in the line to identify date components
                    words = line.split()
                    for word in words:
                        word = word.strip(",")
                        if self._is_month(word):
                            month = en_month_mapping.get(word.lower())
                        elif word.isdigit() and len(word) <= 2:
                            day_str = word
                        elif self._is_year(word):
                            year_str = word

                    # Check if all date components are present
                    if month is not None and day_str is not None and year_str is not None:
                        # Parsing and reformatting the date
                        date_object = datetime.datetime.strptime(f"{year_str}-{month:02d}-{int(day_str):02d}",
                                                                 '%Y-%m-%d')
                        date = date_object.strftime('%d/%m/%Y')

                    # Check if any France-related word is present in the title

                        varadkar_related_title = True
                        duplicate_title = False

                if line == 'Body':
                    # Initialize the text list for the article and set in_body to True
                    text = []
                    in_body = True
                    # Use continue key word to skip adding word Body to text list.
                    continue
                if in_body:
                    # the date the article was uploaded "load date" indicated the end of the article
                    if line_index + 1 < num_lines and "Load-Date:" in lines[line_index+1]:
                        # Only add unique articles with France related keywords in title.
                        if varadkar_related_title and not duplicate_title:
                            #tokenize text when building corpus to avoid having to do it every time the SA is run
                            # Tokenize the text using spacy NLP model
                            text = " ".join(text)
                            tokens = self.tokenise(text, nlp)

                            # Data frame object representation of article.
                            article = {
                                "Date": date,
                                "Source": newspaper,
                                "Text": tokens,
                            }
                            articles.append(article)
                            varadkar_related_title = False
                            in_body = False
                    else:
                        # Append lines to the text list if inside 'Body'
                        text.append(line)
        return articles

    def create_corpus(self, file_names):
        corpus = []
        # NLP language model trained on english news data.
        nlp = spacy.load("en_core_web_sm")

        for fn in file_names:
            #list of data frames
            articles = self._files_to_corpus(fn, nlp)
            corpus.extend(articles)
        # Get corpus from news media
        time_series_corpus = pd.DataFrame(corpus)
        # Convert lists of tokens to strings
        time_series_corpus = time_series_corpus[['Date', 'Source', 'Text']]
        print(time_series_corpus.columns)
        oireachtas_df = pd.read_csv("./DATA/Leo-Varadkar.D.2007-06-14_limit5000.tsv", sep='\t', nrows=500)
        # merge dÃ¡il time series and news media time series
        # match date formats of oireachtas and lexisnexis
        # Date (date): XX/XX/XXXX | Source: dail | Text(contribution): hksdjhgsdhz
        time_series_corpus['Date'] = pd.to_datetime(time_series_corpus['Date'], format='%d/%m/%Y').dt.strftime(
            '%Y-%m-%d')
        oireachtas_df = oireachtas_df.rename(columns={'date': 'Date'})

        # tokenize contriubutions in oireachtas_df
        oireachtas_df['Text'] = oireachtas_df['contribution'].apply(lambda text: self.tokenise(text, nlp))
        oireachtas_df = oireachtas_df.rename(columns={'house_code': 'Source'})


        time_series_corpus['Text'] = time_series_corpus['Text'].apply(lambda tokens: ' '.join(tokens))
        oireachtas_df['Text'] = oireachtas_df['Text'].apply(lambda tokens: ' '.join(tokens))
        oireachtas_df = oireachtas_df[['Date', 'Source', 'Text']]

        # Merge the two DataFrames based on Date, Source, and Text columns
        merged_df = pd.merge(time_series_corpus, oireachtas_df, on=['Date', 'Source', 'Text'], how='outer')

        # Add corpus from Oireachtas.
        return merged_df


# Define the data location
data_location = "C:/Users/josep/PycharmProjects/TD_Language_Analysis/DATA"

# Create an instance of CorpusLexisNexis
corpus = DataToCorpus(data_location)

# Get the file names
file_names = os.listdir(data_location)

# Create the corpus
time_series_corpus = corpus.create_corpus(file_names)


# Output the DataFrame to a TSV file
output_file = "corpus_output.tsv"
time_series_corpus.to_csv(output_file, sep='\t', index=False)

print(f"Corpus output saved to {output_file}")


